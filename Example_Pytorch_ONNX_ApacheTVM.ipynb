{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3148c019",
   "metadata": {},
   "source": [
    "*MEMO :: this codes based on tvm docker *\n",
    "\n",
    "1)install cuda docker\n",
    "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker\n",
    "\n",
    "\n",
    "```\n",
    "(install nvidia driver first, )\n",
    "$>distribution=$(. /etc/os-release;echo $ID$VERSION_ID)    && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -    && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n",
    "\n",
    "$>curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list\n",
    "\n",
    "$>sudo apt-get update\n",
    "$>sudo apt-get install -y nvidia-docker2\n",
    "$>sudo systemctl restart docker\n",
    "$>sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\n",
    "```\n",
    "\n",
    "2)build tvm docker\n",
    "https://tvm.apache.org/docs/install/docker.html\n",
    "```\n",
    "$>git clone --recursive https://github.com/apache/tvm tvm\n",
    "$>cd tvm\n",
    "$>sudo ./docker/build.sh ci_gpu make -j$(nproc)\n",
    "$>sudo ./docker/bash.sh ci_gpu\n",
    "\n",
    "(gpu is unbind to pytorch. re-tinstall torch)\n",
    "$>sudo  pip3 uninstall torch torchvision torchaudio\n",
    "$>pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "(build tvm with LLVM&Cuda)\n",
    "$>mkdir build\n",
    "$>cd build\n",
    "$>cp ../cmake/config.cmake ./\n",
    "$>sudo apt install vim\n",
    "$>vi config.cmake\n",
    "-----change these settings\n",
    "set(USE_CUDA ON)\n",
    "set(USE_LLVM ON)\n",
    "------\n",
    "$>cmake ..\n",
    "$>make\n",
    "\n",
    "$>cd ../\n",
    "\n",
    "-----install jupyter, while upgrading pygments\n",
    "sudo pip install --upgrade pip\n",
    "sudo pip install -U pygments\n",
    "sudo pip install jupyter\n",
    "\n",
    "\n",
    "$> git clone this repository\n",
    "$>.local/bin/jupyter notebook Example_Pytorch_ONNX_ApacheTVM/example_onnx_tvm_mnist.onnx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-hierarchy",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "increased-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-world",
   "metadata": {},
   "source": [
    "CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qualified-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-presentation",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "combined-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch , log_interval , dry_run):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-sociology",
   "metadata": {},
   "source": [
    "Fast Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-visibility",
   "metadata": {},
   "source": [
    "Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "orange-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_eneble = True\n",
    "batch_size      = 64\n",
    "test_batch_size = 1000\n",
    "epochs          = 10\n",
    "lr              = 0.8\n",
    "lr_gamma        = 0.7\n",
    "dry_run         = False\n",
    "seed            = 1\n",
    "log_interval    = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-hughes",
   "metadata": {},
   "source": [
    "exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaging-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_enable:: True\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.283075\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.849216\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.901930\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.636304\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.437891\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.423375\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.167763\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.513060\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.256077\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.162157\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.235267\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.193494\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.286783\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.237601\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.325295\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.137223\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.272183\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.122451\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.298786\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.124014\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.102916\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.190535\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.130047\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.142970\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.315155\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.356444\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.079982\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.129032\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.229249\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.180873\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.217460\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.172561\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.202573\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.066105\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.342712\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.151119\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.382418\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.111466\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.052448\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.121952\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.131895\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.282826\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.190729\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.027246\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.180036\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.054369\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.074701\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.107800\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.203754\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.152375\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.167974\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.069604\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.064941\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.224161\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.073887\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.052177\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.099231\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.178433\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.135618\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.067657\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.104656\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.060876\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.395655\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.162755\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.069753\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.139649\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.125921\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.065146\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.087473\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.074911\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.119014\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.244933\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.115682\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.128591\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.017448\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.082358\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.167310\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.091375\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.095299\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.057261\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.213019\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.098286\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.183874\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.054611\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.039626\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.108536\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.246672\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.083489\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.182516\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.127909\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.065419\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.085419\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.059142\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.068024\n",
      "\n",
      "Test set: Average loss: 0.0524, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.129048\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.027791\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.127514\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.192097\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.079719\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.046613\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.201801\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.110134\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.074298\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.014579\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.145829\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.032255\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.008528\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.055649\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.042386\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.093460\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.136326\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.065922\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.071993\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.053227\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.062932\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.033671\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.232293\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.016321\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.216342\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.013460\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.105609\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.020878\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.049356\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.105805\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.022173\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.110164\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.117098\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.033303\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.019857\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.041168\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.009860\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.169734\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.003155\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.140794\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.115111\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.098605\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.036236\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.064082\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.018576\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.319227\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.119030\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.138846\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.021975\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.052596\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.031863\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.081640\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.012435\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.056551\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.119988\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.006293\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.004715\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.054523\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.093261\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.035262\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.022794\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.006071\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.032232\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.009905\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.105519\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.066381\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.036142\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.018688\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.003666\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.007395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.096310\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.015963\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.157183\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.166444\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.125286\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.157127\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.157839\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.028343\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.024648\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.030298\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.029308\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.006858\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.072286\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.308336\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.125242\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.010060\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.008728\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.133744\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.017119\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.104869\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.048255\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.082872\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.014694\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.090724\n",
      "\n",
      "Test set: Average loss: 0.0347, Accuracy: 9878/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.066313\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.041213\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.104720\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.039349\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.015945\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.029867\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.017411\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.077198\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.023087\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.035046\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.037873\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.015746\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.039625\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.034475\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.029593\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.087991\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.001396\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.129643\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.020810\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.006706\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.011138\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.019554\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.094288\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.029791\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.049473\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.318987\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.060935\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.068578\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.146083\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.075194\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.066451\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.044568\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.025057\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.015643\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.068568\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.127565\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.142922\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.036047\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.006048\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.110116\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.005333\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.011916\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.011007\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.047194\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.044388\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.143830\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.032407\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.276114\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.050587\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.031205\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.220882\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.010793\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.002314\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.013524\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.005007\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.070057\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.161270\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.012850\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.271301\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.222219\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.067869\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.086173\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.014733\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.042351\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.032947\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.108428\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.072445\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.089183\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.005474\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.035950\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.038387\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.046760\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.023606\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.009626\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.056111\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.021711\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.158816\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.025985\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.021993\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.046669\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.076628\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.004410\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.065608\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.128769\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.144894\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.029817\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.178699\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.040126\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.184213\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.003491\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.013976\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.059785\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.019493\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.007765\n",
      "\n",
      "Test set: Average loss: 0.0365, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.022552\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.021689\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.132343\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.032089\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.044445\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.054763\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.043471\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.009066\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.088173\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.014665\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.000622\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.127832\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.003800\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.021906\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.063483\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.015108\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.005928\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.022507\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.007434\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.103605\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.111014\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.024378\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.082376\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.010986\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.001431\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.005133\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.010711\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.032076\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.022239\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.044741\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.007983\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.002329\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.070323\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.007075\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.004857\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.046219\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.022115\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.026904\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.011376\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.036433\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.028166\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.054279\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.006483\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.116663\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.045338\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.106076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.011649\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.070671\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.155299\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.008003\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.016584\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.067238\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.104872\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.003354\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.008735\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.010915\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.005601\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.088036\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.006265\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.012163\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.009096\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.150230\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.004652\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.008912\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.009325\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.004879\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.031425\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.013427\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.035639\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.022697\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.003005\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.022185\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.010351\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.003589\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.036136\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.046689\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.085283\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.025400\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.014010\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.006957\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.042642\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.011068\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.109410\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.005433\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.044768\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.037179\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.016679\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.017131\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.007248\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.011549\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.038795\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.028276\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.041467\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.173889\n",
      "\n",
      "Test set: Average loss: 0.0305, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.005730\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.024551\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.014703\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.003838\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.003827\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.039415\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.112116\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.025068\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.006425\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.074613\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.008201\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.001565\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.002660\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.065989\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.012860\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.013711\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.010058\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.045784\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.035769\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.012317\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.018116\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.154625\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.015493\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.002452\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.101800\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.071369\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.006403\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.027462\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.028600\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.084297\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.016323\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.017817\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.057035\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.008249\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.100221\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.143541\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.051740\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.045956\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.018070\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.002531\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.024033\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.013122\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.145411\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.055461\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.018779\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.001972\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.005683\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.055936\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.000853\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.065106\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.150634\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.006671\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.086425\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.012057\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.040897\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.037291\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.023811\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.002476\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.001337\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.026861\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.059547\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.002318\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.026849\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.041026\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.014089\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.031023\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.003774\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.022326\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.058907\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.000656\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.016413\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.017411\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.010687\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.030514\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.016944\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.028332\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.012148\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.045450\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.031935\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.028071\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.039692\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.009845\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.024347\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.091095\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.010712\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.040155\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.003528\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.005100\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.005170\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.079648\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.024440\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.041136\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.002052\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.012212\n",
      "\n",
      "Test set: Average loss: 0.0284, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.026690\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.039217\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.199333\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.230392\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.060509\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.113569\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.007345\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.009455\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.003021\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.011436\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.012267\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.038002\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.011292\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.059795\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.060920\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.029054\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.032899\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.017851\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.011544\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.001147\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.063228\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.033779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.018145\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.017905\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.086053\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.020127\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.007943\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.001304\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.010778\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.055777\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.041713\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.003600\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.158085\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.001315\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.011918\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.003283\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.013119\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.005471\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.007696\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.026490\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.006918\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.078414\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.003105\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.011989\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.006441\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.023384\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.030654\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.071902\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.033243\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.084834\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.048477\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.152127\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.018372\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.003009\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.016765\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.035758\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.065812\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.016636\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.005116\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.001418\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.019802\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.023038\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.038980\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.014371\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.073435\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.155981\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.034446\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.059354\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.002197\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.016048\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.013991\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.059299\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.095218\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.047527\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.005606\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.005775\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.011663\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.054688\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.023585\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.058959\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.041192\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.004241\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.003610\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.031721\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.092521\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.058482\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.029445\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.040993\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.045356\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.008773\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.017636\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.013157\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.028479\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.065542\n",
      "\n",
      "Test set: Average loss: 0.0291, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.006869\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.338554\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.015547\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.001265\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.124153\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.158204\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.025935\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.002777\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.051720\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.002510\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.003136\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.037703\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.006400\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.020362\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.004107\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.000921\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.014397\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.010670\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.056944\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.177336\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.026306\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.002127\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.066833\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.018695\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.005236\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.001611\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.004123\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.022256\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.160817\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.030307\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.008892\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.020893\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.001929\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.001219\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.039460\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.006610\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.075787\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.007453\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.011063\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.046277\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.047449\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.039644\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.004460\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.000787\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.008253\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.002400\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.013765\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.174453\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.030526\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.001869\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.003630\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.129654\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.010732\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.004600\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.002491\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.006976\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.001764\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.017094\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.000553\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.069056\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.032720\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.035297\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.071088\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.025399\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.022218\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.003149\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.007105\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.020779\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.100344\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.012487\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.007469\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.081593\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.004176\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.038722\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.055207\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.003148\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.066461\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.054495\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.024207\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.011518\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.008763\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.068570\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.024923\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.001721\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.001270\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.009368\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.016156\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.005991\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.006000\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.151562\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.016623\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.007508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.136345\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.076817\n",
      "\n",
      "Test set: Average loss: 0.0281, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.083414\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.002175\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.001000\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.001771\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.012994\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.002224\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.002534\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.000488\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.004750\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.002831\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.007035\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.054209\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.000559\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.002569\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.011860\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.001101\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.050110\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.040408\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.059074\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.011256\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.012504\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.053665\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.011399\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.012402\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.001738\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.016396\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.008852\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.029140\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.008124\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.000529\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.002315\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.002740\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.004362\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.005499\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.026690\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.002138\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.005582\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.010784\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.196033\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.009675\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006909\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.121826\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.055064\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.101389\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.086952\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.020753\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.031010\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.072168\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.010741\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.066621\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006800\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.030928\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.019079\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.126331\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.014781\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.011800\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.002234\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.045766\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.036435\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.001170\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.002706\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.012486\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.232262\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.045518\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.134111\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.022326\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.343972\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.007671\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.006363\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.022737\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.008594\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.039441\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.057630\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.001231\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.039186\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.007111\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.023706\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.139728\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.012096\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.014030\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.010181\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.004395\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.001006\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.007598\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.213336\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.034561\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.002699\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.001446\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.002182\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.003148\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.004028\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.006139\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.039887\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.016716\n",
      "\n",
      "Test set: Average loss: 0.0273, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.011791\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.011951\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.006332\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.008627\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.002784\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.001002\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.014231\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.001051\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.001778\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.042946\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.013127\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.001232\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.004286\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.000273\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.020417\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.070538\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.114461\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.015354\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.002011\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.005817\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.008715\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.023559\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.053780\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.004716\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.031939\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.006400\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.022133\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.015940\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.045320\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.208279\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.022818\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.185278\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.004247\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.006287\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.037809\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.001293\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.057903\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.011275\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.095129\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.028270\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.036938\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.018319\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.003566\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.147638\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.007064\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.004993\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.016174\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.010907\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.003202\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.002415\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.003785\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.007990\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.005032\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.016139\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.005913\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.001680\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.073347\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.001684\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.049776\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.100225\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.051405\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.005187\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.002516\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.024256\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.017605\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.033629\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.080065\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.028528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.004819\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.005608\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.001201\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.160222\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.045596\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.000934\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.003717\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.004623\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.018653\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.001308\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.253937\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.143329\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.005685\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.001407\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.004758\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.023108\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.010035\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.066937\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.003255\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.005732\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.007310\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.001233\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.083841\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.016556\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.007853\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.002433\n",
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.017634\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.001948\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.006110\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.014340\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.001497\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.004260\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.005416\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.000909\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.010316\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.014648\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.039511\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.028394\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.113165\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.034457\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.025867\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.010257\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.064225\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.002386\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.039781\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.004676\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001862\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.022413\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.002760\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.006929\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.012339\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.003847\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.045934\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.008306\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.054006\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.062164\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.008120\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.023475\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.074310\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.002871\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.017894\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.039761\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.006907\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.079969\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.002622\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.032915\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.014973\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.005366\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.025650\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.000625\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.000984\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.021608\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.053260\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.084190\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.083928\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.042036\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.002269\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.004421\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.038032\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.004549\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.001619\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.007526\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.106342\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.066760\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.002737\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.054358\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.007306\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.002643\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.023625\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.001118\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.000265\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.006479\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.007267\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.004593\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.053231\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.001989\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.005062\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.088124\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.042978\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.106229\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.016472\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.004113\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.006101\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.000746\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.013811\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.006136\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000313\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.006536\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.004723\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.059616\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.001457\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.010082\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.001421\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.039602\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.067393\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.003853\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.032136\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.071114\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.001907\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.009669\n",
      "\n",
      "Test set: Average loss: 0.0266, Accuracy: 9918/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if training_eneble:\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    print(\"gpu_enable::\",use_cuda)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    dataset1 = datasets.MNIST('./data', train=True, download=True,transform=transform)\n",
    "    dataset2 = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=lr_gamma )\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch,log_interval , dry_run)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e13c1c",
   "metadata": {},
   "source": [
    "Save as ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98685d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%actual_input_1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cuda:0),\n",
      "      %conv1.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %conv1.bias : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %conv2.weight : Float(64, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %conv2.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.weight : Float(128, 9216, strides=[9216, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.weight : Float(10, 128, strides=[128, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(10, strides=[1], requires_grad=1, device=cuda:0)):\n",
      "  %x : Float(1, 32, 26, 26, strides=[21632, 676, 26, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%actual_input_1, %conv1.weight, %conv1.bias) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py:444:0\n",
      "  %input : Float(1, 32, 26, 26, strides=[21632, 676, 26, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%x) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/functional.py:1442:0\n",
      "  %x.3 : Float(1, 64, 24, 24, strides=[36864, 576, 24, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input, %conv2.weight, %conv2.bias) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py:444:0\n",
      "  %12 : Float(1, 64, 24, 24, strides=[36864, 576, 24, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%x.3) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/functional.py:1442:0\n",
      "  %input.4 : Float(1, 64, 12, 12, strides=[9216, 144, 12, 1], requires_grad=1, device=cuda:0) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%12) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/functional.py:797:0\n",
      "  %14 : Float(1, 9216, strides=[9216, 1], requires_grad=1, device=cuda:0) = onnx::Flatten[axis=1](%input.4) # /tmp/ipykernel_26442/966138162.py:18:0\n",
      "  %x.7 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%14, %fc1.weight, %fc1.bias) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py:103:0\n",
      "  %input.8 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = onnx::Relu(%x.7) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/functional.py:1442:0\n",
      "  %x.11 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cuda:0) = onnx::Gemm[alpha=1., beta=1., transB=1](%input.8, %fc2.weight, %fc2.bias) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py:103:0\n",
      "  %output1 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cuda:0) = onnx::LogSoftmax[axis=1](%x.11) # /home/tommy/work_study/tvm/.local/lib/python3.7/site-packages/torch/nn/functional.py:1907:0\n",
      "  return (%output1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "# Providing input and output names sets the display names for values\n",
    "# within the model's graph. Setting these does not change the semantics\n",
    "# of the graph; it is only for readability.\n",
    "#\n",
    "# The inputs to the network consist of the flat list of inputs (i.e.\n",
    "# the values you would pass to the forward() method) followed by the\n",
    "# flat list of parameters. You can partially specify names, i.e. provide\n",
    "# a list here shorter than the number of inputs to the model, and we will\n",
    "# only set that subset of names, starting from the beginning.\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device=\"cuda\")\n",
    "input_names = [ \"actual_input_1\" ] #+ [ \"learned_%d\" % i for i in range(8) ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"example_onnx_tvm_mnist.onnx\", verbose=True, input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81dde7",
   "metadata": {},
   "source": [
    "Check loaded-NN is actually working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df00eb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target= 2\n",
      "infer_res= tensor([2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##ONNX load is not supported in pytorch\n",
    "#re-load model from .pt for TVM check\n",
    "model_infer = Net()\n",
    "model_infer.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "\n",
    "##check loaded model is working\n",
    "import matplotlib.pyplot as plt\n",
    "sample0, ref = dataset2[1]\n",
    "plt.imshow(sample0.numpy()[0], cmap='gray')\n",
    "print(\"target=\",ref)\n",
    "sample0 = sample0.view(1,1,28,28) #add batch1 dimension\n",
    "infer_res = model_infer(sample0)\n",
    "print(\"infer_res=\",infer_res.argmax(dim=1, keepdim=False))\n",
    "\n",
    "check_all_eneble = False\n",
    "if check_all_eneble:\n",
    "    test_batch1_kwargs = {'batch_size': 1}\n",
    "    test_batch1_loader = torch.utils.data.DataLoader(dataset2, **test_batch1_kwargs)\n",
    "    for data, target in test_batch1_loader:\n",
    "        output = model_infer(data)\n",
    "        pred = output.argmax(dim=1, keepdim=False)\n",
    "        print(\"infer\" , pred)\n",
    "        print(\"ref\" , target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7d526",
   "metadata": {},
   "source": [
    "ONNX Load NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "indonesian-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph torch-jit-export (\n",
      "  %actual_input_1[FLOAT, 1x1x28x28]\n",
      ") initializers (\n",
      "  %conv1.weight[FLOAT, 32x1x3x3]\n",
      "  %conv1.bias[FLOAT, 32]\n",
      "  %conv2.weight[FLOAT, 64x32x3x3]\n",
      "  %conv2.bias[FLOAT, 64]\n",
      "  %fc1.weight[FLOAT, 128x9216]\n",
      "  %fc1.bias[FLOAT, 128]\n",
      "  %fc2.weight[FLOAT, 10x128]\n",
      "  %fc2.bias[FLOAT, 10]\n",
      ") {\n",
      "  %x = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [1, 1]](%actual_input_1, %conv1.weight, %conv1.bias)\n",
      "  %input = Relu(%x)\n",
      "  %x.3 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [1, 1]](%input, %conv2.weight, %conv2.bias)\n",
      "  %onnx::MaxPool_12 = Relu(%x.3)\n",
      "  %input.4 = MaxPool[kernel_shape = [2, 2], pads = [0, 0, 0, 0], strides = [2, 2]](%onnx::MaxPool_12)\n",
      "  %onnx::Gemm_14 = Flatten[axis = 1](%input.4)\n",
      "  %x.7 = Gemm[alpha = 1, beta = 1, transB = 1](%onnx::Gemm_14, %fc1.weight, %fc1.bias)\n",
      "  %input.8 = Relu(%x.7)\n",
      "  %x.11 = Gemm[alpha = 1, beta = 1, transB = 1](%input.8, %fc2.weight, %fc2.bias)\n",
      "  %output1 = LogSoftmax[axis = 1](%x.11)\n",
      "  return %output1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "# Load the ONNX model\n",
    "model_onnx = onnx.load(\"example_onnx_tvm_mnist.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model_onnx)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model_onnx.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1be881",
   "metadata": {},
   "source": [
    "TVM compile test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe9d70b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "def @main(%actual_input_1: Tensor[(1, 1, 28, 28), float32]) -> Tensor[(1, 10), float32] {\n",
      "  %0 = nn.conv2d(%actual_input_1, meta[relay.Constant][0] /* ty=Tensor[(32, 1, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 26, 26), float32] */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 26, 26), float32] */;\n",
      "  %2 = nn.relu(%1) /* ty=Tensor[(1, 32, 26, 26), float32] */;\n",
      "  %3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 24, 24), float32] */;\n",
      "  %4 = nn.bias_add(%3, meta[relay.Constant][3] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 24, 24), float32] */;\n",
      "  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 24, 24), float32] */;\n",
      "  %6 = nn.max_pool2d(%5, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 12, 12), float32] */;\n",
      "  %7 = nn.batch_flatten(%6) /* ty=Tensor[(1, 9216), float32] */;\n",
      "  %8 = nn.dense(%7, meta[relay.Constant][4] /* ty=Tensor[(128, 9216), float32] */, units=128) /* ty=Tensor[(1, 128), float32] */;\n",
      "  %9 = add(%8, meta[relay.Constant][5] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(1, 128), float32] */;\n",
      "  %10 = nn.relu(%9) /* ty=Tensor[(1, 128), float32] */;\n",
      "  %11 = nn.dense(%10, meta[relay.Constant][6] /* ty=Tensor[(10, 128), float32] */, units=10) /* ty=Tensor[(1, 10), float32] */;\n",
      "  %12 = add(%11, meta[relay.Constant][7] /* ty=Tensor[(10), float32] */) /* ty=Tensor[(1, 10), float32] */;\n",
      "  %13 = max(%12, axis=[1], keepdims=True) /* ty=Tensor[(1, 1), float32] */;\n",
      "  %14 = subtract(%12, %13) /* ty=Tensor[(1, 10), float32] */;\n",
      "  %15 = exp(%14) /* ty=Tensor[(1, 10), float32] */;\n",
      "  %16 = sum(%15, axis=[1], keepdims=True) /* ty=Tensor[(1, 1), float32] */;\n",
      "  %17 = subtract(%12, %13) /* ty=Tensor[(1, 10), float32] */;\n",
      "  %18 = log(%16) /* ty=Tensor[(1, 1), float32] */;\n",
      "  subtract(%17, %18) /* ty=Tensor[(1, 10), float32] */\n",
      "}\n",
      "\n",
      "/* For debugging purposes the metadata section has been omitted.\n",
      " * If you would like to see the full metadata section you can set the \n",
      " * option to `True` when invoking `astext`. \n",
      " */\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "import tvm.relay as relay\n",
    "from tvm.contrib.download import download_testdata\n",
    "from tvm.contrib import graph_executor\n",
    "import numpy as np\n",
    "\n",
    "input_name = \"actual_input_1\"\n",
    "\n",
    "shape_dict = {input_name: dummy_input.shape}\n",
    "mod, params = relay.frontend.from_onnx(model_onnx, shape_dict)\n",
    "\n",
    "#initial Relay IR\n",
    "print(mod.astext(show_meta_data=False))\n",
    "\n",
    "target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=1):\n",
    "    lib = relay.build(mod, target=target, params=params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75b0250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n",
      "#define __shfl_sync(mask, var, lane, width) \\\n",
      "        __shfl((var), (lane), (width))\n",
      "\n",
      "#define __shfl_down_sync(mask, var, offset, width) \\\n",
      "        __shfl_down((var), (offset), (width))\n",
      "\n",
      "#define __shfl_up_sync(mask, var, offset, width) \\\n",
      "        __shfl_up((var), (offset), (width))\n",
      "#endif\n",
      "\n",
      "\n",
      "#ifdef _WIN32\n",
      "  using uint = unsigned int;\n",
      "  using uchar = unsigned char;\n",
      "  using ushort = unsigned short;\n",
      "  using int64_t = long long;\n",
      "  using uint64_t = unsigned long long;\n",
      "#else\n",
      "  #define uint unsigned int\n",
      "  #define uchar unsigned char\n",
      "  #define ushort unsigned short\n",
      "  #define int64_t long long\n",
      "  #define uint64_t unsigned long long\n",
      "#endif\n",
      "extern \"C\" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_max_pool2d_kernel0(float* __restrict__ placeholder, float* __restrict__ tensor) {\n",
      "  float tensor_local[1];\n",
      "  tensor_local[0] = -3.402823e+38f;\n",
      "  for (int rv0 = 0; rv0 < 2; ++rv0) {\n",
      "    for (int rv1 = 0; rv1 < 2; ++rv1) {\n",
      "      tensor_local[0] = max(tensor_local[0], placeholder[(((((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) >> 2)) / 3) * 48) + (rv0 * 24)) + ((((((int)blockIdx.x) * 1024) + ((int)threadIdx.x)) % 12) * 2)) + rv1)]);\n",
      "    }\n",
      "  }\n",
      "  tensor[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = tensor_local[0];\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void __launch_bounds__(192) tvmgen_default_fused_nn_conv2d_nn_bias_add_nn_relu_1_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_relu, float* __restrict__ placeholder2) {\n",
      "  float conv2d_nchw[16];\n",
      "  __shared__ float pad_temp_shared[312];\n",
      "  __shared__ float placeholder_shared[576];\n",
      "  for (int yy_init = 0; yy_init < 2; ++yy_init) {\n",
      "    conv2d_nchw[yy_init] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 4)] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 8)] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 12)] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 2)] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 6)] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 10)] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 14)] = 0.000000e+00f;\n",
      "  }\n",
      "  for (int rc_outer = 0; rc_outer < 16; ++rc_outer) {\n",
      "    __syncthreads();\n",
      "    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n",
      "      if (((((((int)threadIdx.x) * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) / 39) + ((int)threadIdx.z)) < 8) {\n",
      "        if (((((int)threadIdx.x) * 2) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) < 39) {\n",
      "          pad_temp_shared[(((((int)threadIdx.z) * 39) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = placeholder[((((rc_outer * 1352) + ((((int)threadIdx.z) >> 2) * 676)) + (((int)blockIdx.y) * 104)) + ((((((int)threadIdx.z) * 39) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) % 156))];\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1 < 3; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1) {\n",
      "      placeholder_shared[(((((int)threadIdx.z) * 72) + (((int)threadIdx.x) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1)] = placeholder1[((((((((int)blockIdx.z) * 9216) + (((int)threadIdx.z) * 1152)) + ((((int)threadIdx.x) / 6) * 288)) + (rc_outer * 18)) + ((((int)threadIdx.x) % 6) * 3)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner1)];\n",
      "    }\n",
      "    __syncthreads();\n",
      "    for (int rc_inner = 0; rc_inner < 2; ++rc_inner) {\n",
      "      for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n",
      "        for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n",
      "          for (int yy = 0; yy < 2; ++yy) {\n",
      "            conv2d_nchw[yy] = (conv2d_nchw[yy] + (pad_temp_shared[(((((rc_inner * 156) + (yy * 26)) + (ry_inner * 26)) + ((int)threadIdx.x)) + rx_inner)] * placeholder_shared[((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner)]));\n",
      "            conv2d_nchw[(yy + 4)] = (conv2d_nchw[(yy + 4)] + (pad_temp_shared[(((((rc_inner * 156) + (yy * 26)) + (ry_inner * 26)) + ((int)threadIdx.x)) + rx_inner)] * placeholder_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 144)]));\n",
      "            conv2d_nchw[(yy + 8)] = (conv2d_nchw[(yy + 8)] + (pad_temp_shared[(((((rc_inner * 156) + (yy * 26)) + (ry_inner * 26)) + ((int)threadIdx.x)) + rx_inner)] * placeholder_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 288)]));\n",
      "            conv2d_nchw[(yy + 12)] = (conv2d_nchw[(yy + 12)] + (pad_temp_shared[(((((rc_inner * 156) + (yy * 26)) + (ry_inner * 26)) + ((int)threadIdx.x)) + rx_inner)] * placeholder_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 432)]));\n",
      "            conv2d_nchw[(yy + 2)] = (conv2d_nchw[(yy + 2)] + (pad_temp_shared[((((((rc_inner * 156) + (yy * 26)) + (ry_inner * 26)) + ((int)threadIdx.x)) + rx_inner) + 52)] * placeholder_shared[((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner)]));\n",
      "            conv2d_nchw[(yy + 6)] = (conv2d_nchw[(yy + 6)] + (pad_temp_shared[((((((rc_inner * 156) + (yy * 26)) + (ry_inner * 26)) + ((int)threadIdx.x)) + rx_inner) + 52)] * placeholder_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 144)]));\n",
      "            conv2d_nchw[(yy + 10)] = (conv2d_nchw[(yy + 10)] + (pad_temp_shared[((((((rc_inner * 156) + (yy * 26)) + (ry_inner * 26)) + ((int)threadIdx.x)) + rx_inner) + 52)] * placeholder_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 288)]));\n",
      "            conv2d_nchw[(yy + 14)] = (conv2d_nchw[(yy + 14)] + (pad_temp_shared[((((((rc_inner * 156) + (yy * 26)) + (ry_inner * 26)) + ((int)threadIdx.x)) + rx_inner) + 52)] * placeholder_shared[(((((((int)threadIdx.z) * 18) + (rc_inner * 9)) + (ry_inner * 3)) + rx_inner) + 432)]));\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  for (int ax2_inner_inner_inner = 0; ax2_inner_inner_inner < 2; ++ax2_inner_inner_inner) {\n",
      "    T_relu[(((((((int)blockIdx.z) * 18432) + (((int)threadIdx.z) * 576)) + (((int)blockIdx.y) * 96)) + (ax2_inner_inner_inner * 24)) + ((int)threadIdx.x))] = max((conv2d_nchw[ax2_inner_inner_inner] + placeholder2[((((int)blockIdx.z) * 32) + ((int)threadIdx.z))]), 0.000000e+00f);\n",
      "    T_relu[((((((((int)blockIdx.z) * 18432) + (((int)threadIdx.z) * 576)) + (((int)blockIdx.y) * 96)) + (ax2_inner_inner_inner * 24)) + ((int)threadIdx.x)) + 4608)] = max((conv2d_nchw[(ax2_inner_inner_inner + 4)] + placeholder2[(((((int)blockIdx.z) * 32) + ((int)threadIdx.z)) + 8)]), 0.000000e+00f);\n",
      "    T_relu[((((((((int)blockIdx.z) * 18432) + (((int)threadIdx.z) * 576)) + (((int)blockIdx.y) * 96)) + (ax2_inner_inner_inner * 24)) + ((int)threadIdx.x)) + 9216)] = max((conv2d_nchw[(ax2_inner_inner_inner + 8)] + placeholder2[(((((int)blockIdx.z) * 32) + ((int)threadIdx.z)) + 16)]), 0.000000e+00f);\n",
      "    T_relu[((((((((int)blockIdx.z) * 18432) + (((int)threadIdx.z) * 576)) + (((int)blockIdx.y) * 96)) + (ax2_inner_inner_inner * 24)) + ((int)threadIdx.x)) + 13824)] = max((conv2d_nchw[(ax2_inner_inner_inner + 12)] + placeholder2[(((((int)blockIdx.z) * 32) + ((int)threadIdx.z)) + 24)]), 0.000000e+00f);\n",
      "    T_relu[((((((((int)blockIdx.z) * 18432) + (((int)threadIdx.z) * 576)) + (((int)blockIdx.y) * 96)) + (ax2_inner_inner_inner * 24)) + ((int)threadIdx.x)) + 48)] = max((conv2d_nchw[(ax2_inner_inner_inner + 2)] + placeholder2[((((int)blockIdx.z) * 32) + ((int)threadIdx.z))]), 0.000000e+00f);\n",
      "    T_relu[((((((((int)blockIdx.z) * 18432) + (((int)threadIdx.z) * 576)) + (((int)blockIdx.y) * 96)) + (ax2_inner_inner_inner * 24)) + ((int)threadIdx.x)) + 4656)] = max((conv2d_nchw[(ax2_inner_inner_inner + 6)] + placeholder2[(((((int)blockIdx.z) * 32) + ((int)threadIdx.z)) + 8)]), 0.000000e+00f);\n",
      "    T_relu[((((((((int)blockIdx.z) * 18432) + (((int)threadIdx.z) * 576)) + (((int)blockIdx.y) * 96)) + (ax2_inner_inner_inner * 24)) + ((int)threadIdx.x)) + 9264)] = max((conv2d_nchw[(ax2_inner_inner_inner + 10)] + placeholder2[(((((int)blockIdx.z) * 32) + ((int)threadIdx.z)) + 16)]), 0.000000e+00f);\n",
      "    T_relu[((((((((int)blockIdx.z) * 18432) + (((int)threadIdx.z) * 576)) + (((int)blockIdx.y) * 96)) + (ax2_inner_inner_inner * 24)) + ((int)threadIdx.x)) + 13872)] = max((conv2d_nchw[(ax2_inner_inner_inner + 14)] + placeholder2[(((((int)blockIdx.z) * 32) + ((int)threadIdx.z)) + 24)]), 0.000000e+00f);\n",
      "  }\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void __launch_bounds__(64) tvmgen_default_fused_nn_dense_add_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_add, float* __restrict__ placeholder2) {\n",
      "  float T_matmul_NT_rf[1];\n",
      "  __shared__ float red_buf0[64];\n",
      "  __shared__ float T_matmul_NT[1];\n",
      "  T_matmul_NT_rf[0] = 0.000000e+00f;\n",
      "  for (int k_outer = 0; k_outer < 2; ++k_outer) {\n",
      "    T_matmul_NT_rf[0] = (T_matmul_NT_rf[0] + (placeholder[((k_outer * 64) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 128) + (k_outer * 64)) + ((int)threadIdx.x))]));\n",
      "  }\n",
      "  __syncthreads();\n",
      "  ((volatile float*)red_buf0)[((int)threadIdx.x)] = T_matmul_NT_rf[0];\n",
      "  __syncthreads();\n",
      "  if (((int)threadIdx.x) < 32) {\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 32)]);\n",
      "  }\n",
      "  __syncthreads();\n",
      "  if (((int)threadIdx.x) < 16) {\n",
      "    float w_16_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 16)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_16_0;\n",
      "    float w_8_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 8)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_8_0;\n",
      "    float w_4_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 4)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_4_0;\n",
      "    float w_2_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 2)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_2_0;\n",
      "    float w_1_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 1)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;\n",
      "  }\n",
      "  __syncthreads();\n",
      "  if (((int)threadIdx.x) == 0) {\n",
      "    T_matmul_NT[0] = ((volatile float*)red_buf0)[0];\n",
      "  }\n",
      "  if (((int)threadIdx.x) == 0) {\n",
      "    T_add[((int)blockIdx.x)] = (T_matmul_NT[0] + placeholder2[((int)blockIdx.x)]);\n",
      "  }\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void __launch_bounds__(208) tvmgen_default_fused_nn_conv2d_nn_bias_add_nn_relu_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_relu, float* __restrict__ placeholder2) {\n",
      "  float conv2d_nchw[8];\n",
      "  __shared__ float pad_temp_shared[112];\n",
      "  __shared__ float placeholder_shared[288];\n",
      "  for (int yy_init = 0; yy_init < 2; ++yy_init) {\n",
      "    conv2d_nchw[yy_init] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 2)] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 4)] = 0.000000e+00f;\n",
      "    conv2d_nchw[(yy_init + 6)] = 0.000000e+00f;\n",
      "  }\n",
      "  if (((((int)threadIdx.x) / 14) + ((int)threadIdx.z)) < 8) {\n",
      "    if (((int)threadIdx.x) < 14) {\n",
      "      pad_temp_shared[((((int)threadIdx.z) * 14) + ((int)threadIdx.x))] = placeholder[(((((int)blockIdx.y) * 56) + (((int)threadIdx.z) * 14)) + ((int)threadIdx.x))];\n",
      "    }\n",
      "  }\n",
      "  for (int ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner = 0; ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner < 2; ++ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner) {\n",
      "    if (((((int)threadIdx.x) / 18) + ((int)threadIdx.z)) < 8) {\n",
      "      if (((int)threadIdx.x) < 18) {\n",
      "        placeholder_shared[(((((int)threadIdx.z) * 36) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)] = placeholder1[(((((int)threadIdx.z) * 36) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_ax3_fused_inner_inner_inner)];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  __syncthreads();\n",
      "  for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n",
      "    for (int rx_inner = 0; rx_inner < 3; ++rx_inner) {\n",
      "      for (int yy = 0; yy < 2; ++yy) {\n",
      "        conv2d_nchw[yy] = (conv2d_nchw[yy] + (pad_temp_shared[((((yy * 28) + (ry_inner * 28)) + ((int)threadIdx.x)) + rx_inner)] * placeholder_shared[(((((int)threadIdx.z) * 9) + (ry_inner * 3)) + rx_inner)]));\n",
      "        conv2d_nchw[(yy + 2)] = (conv2d_nchw[(yy + 2)] + (pad_temp_shared[((((yy * 28) + (ry_inner * 28)) + ((int)threadIdx.x)) + rx_inner)] * placeholder_shared[((((((int)threadIdx.z) * 9) + (ry_inner * 3)) + rx_inner) + 72)]));\n",
      "        conv2d_nchw[(yy + 4)] = (conv2d_nchw[(yy + 4)] + (pad_temp_shared[((((yy * 28) + (ry_inner * 28)) + ((int)threadIdx.x)) + rx_inner)] * placeholder_shared[((((((int)threadIdx.z) * 9) + (ry_inner * 3)) + rx_inner) + 144)]));\n",
      "        conv2d_nchw[(yy + 6)] = (conv2d_nchw[(yy + 6)] + (pad_temp_shared[((((yy * 28) + (ry_inner * 28)) + ((int)threadIdx.x)) + rx_inner)] * placeholder_shared[((((((int)threadIdx.z) * 9) + (ry_inner * 3)) + rx_inner) + 216)]));\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  for (int ax2_inner_inner_inner = 0; ax2_inner_inner_inner < 2; ++ax2_inner_inner_inner) {\n",
      "    T_relu[((((((int)threadIdx.z) * 676) + (((int)blockIdx.y) * 52)) + (ax2_inner_inner_inner * 26)) + ((int)threadIdx.x))] = max((conv2d_nchw[ax2_inner_inner_inner] + placeholder2[((int)threadIdx.z)]), 0.000000e+00f);\n",
      "    T_relu[(((((((int)threadIdx.z) * 676) + (((int)blockIdx.y) * 52)) + (ax2_inner_inner_inner * 26)) + ((int)threadIdx.x)) + 5408)] = max((conv2d_nchw[(ax2_inner_inner_inner + 2)] + placeholder2[(((int)threadIdx.z) + 8)]), 0.000000e+00f);\n",
      "    T_relu[(((((((int)threadIdx.z) * 676) + (((int)blockIdx.y) * 52)) + (ax2_inner_inner_inner * 26)) + ((int)threadIdx.x)) + 10816)] = max((conv2d_nchw[(ax2_inner_inner_inner + 4)] + placeholder2[(((int)threadIdx.z) + 16)]), 0.000000e+00f);\n",
      "    T_relu[(((((((int)threadIdx.z) * 676) + (((int)blockIdx.y) * 52)) + (ax2_inner_inner_inner * 26)) + ((int)threadIdx.x)) + 16224)] = max((conv2d_nchw[(ax2_inner_inner_inner + 6)] + placeholder2[(((int)threadIdx.z) + 24)]), 0.000000e+00f);\n",
      "  }\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void __launch_bounds__(10) tvmgen_default_fused_subtract_log_subtract_kernel0(float* __restrict__ T_subtract, float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ placeholder2) {\n",
      "  T_subtract[((int)threadIdx.x)] = ((placeholder[((int)threadIdx.x)] - placeholder1[0]) - __logf(placeholder2[0]));\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void __launch_bounds__(1024) tvmgen_default_fused_max_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder_red) {\n",
      "  float placeholder_red_rf[1];\n",
      "  float red_buf0[1];\n",
      "  placeholder_red_rf[0] = -3.402823e+38f;\n",
      "  if (((int)threadIdx.y) < 1) {\n",
      "    if (((int)threadIdx.x) < 10) {\n",
      "      placeholder_red_rf[0] = max(placeholder_red_rf[0], placeholder[((((int)threadIdx.y) * 10) + ((int)threadIdx.x))]);\n",
      "    }\n",
      "  }\n",
      "  uint mask[1];\n",
      "  float t0[1];\n",
      "  red_buf0[0] = placeholder_red_rf[0];\n",
      "  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n",
      "  red_buf0[0] = max(red_buf0[0], t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n",
      "  red_buf0[0] = max(red_buf0[0], t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n",
      "  red_buf0[0] = max(red_buf0[0], t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n",
      "  red_buf0[0] = max(red_buf0[0], t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n",
      "  red_buf0[0] = max(red_buf0[0], t0[0]);\n",
      "  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n",
      "  if ((((int)threadIdx.x) == 0) && (((int)threadIdx.y) < 1)) {\n",
      "    placeholder_red[((int)threadIdx.y)] = red_buf0[0];\n",
      "  }\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void __launch_bounds__(1024) tvmgen_default_fused_subtract_exp_sum_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_exp_red) {\n",
      "  float T_exp_red_rf[1];\n",
      "  float red_buf0[1];\n",
      "  T_exp_red_rf[0] = 0.000000e+00f;\n",
      "  if (((int)threadIdx.y) < 1) {\n",
      "    if (((int)threadIdx.x) < 10) {\n",
      "      T_exp_red_rf[0] = (T_exp_red_rf[0] + __expf((placeholder[((((int)threadIdx.y) * 10) + ((int)threadIdx.x))] - placeholder1[((int)threadIdx.y)])));\n",
      "    }\n",
      "  }\n",
      "  uint mask[1];\n",
      "  float t0[1];\n",
      "  red_buf0[0] = T_exp_red_rf[0];\n",
      "  mask[0] = (__activemask() & ((uint)(0 << (((int)threadIdx.y) * 32))));\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n",
      "  red_buf0[0] = (red_buf0[0] + t0[0]);\n",
      "  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], (((int)threadIdx.y) * 32), 32);\n",
      "  if ((((int)threadIdx.x) == 0) && (((int)threadIdx.y) < 1)) {\n",
      "    T_exp_red[((int)threadIdx.y)] = red_buf0[0];\n",
      "  }\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void __launch_bounds__(1024) tvmgen_default_fused_nn_batch_flatten_kernel0(float* __restrict__ tensor, float* __restrict__ placeholder) {\n",
      "  tensor[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))] = placeholder[((((int)blockIdx.x) * 1024) + ((int)threadIdx.x))];\n",
      "}\n",
      "\n",
      "extern \"C\" __global__ void __launch_bounds__(64) tvmgen_default_fused_nn_dense_add_nn_relu_kernel0(float* __restrict__ placeholder, float* __restrict__ placeholder1, float* __restrict__ T_relu, float* __restrict__ placeholder2) {\n",
      "  float T_matmul_NT_rf[1];\n",
      "  __shared__ float red_buf0[64];\n",
      "  __shared__ float T_matmul_NT[1];\n",
      "  T_matmul_NT_rf[0] = 0.000000e+00f;\n",
      "  for (int k_outer = 0; k_outer < 144; ++k_outer) {\n",
      "    T_matmul_NT_rf[0] = (T_matmul_NT_rf[0] + (placeholder[((k_outer * 64) + ((int)threadIdx.x))] * placeholder1[(((((int)blockIdx.x) * 9216) + (k_outer * 64)) + ((int)threadIdx.x))]));\n",
      "  }\n",
      "  __syncthreads();\n",
      "  ((volatile float*)red_buf0)[((int)threadIdx.x)] = T_matmul_NT_rf[0];\n",
      "  __syncthreads();\n",
      "  if (((int)threadIdx.x) < 32) {\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 32)]);\n",
      "  }\n",
      "  __syncthreads();\n",
      "  if (((int)threadIdx.x) < 16) {\n",
      "    float w_16_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 16)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_16_0;\n",
      "    float w_8_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 8)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_8_0;\n",
      "    float w_4_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 4)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_4_0;\n",
      "    float w_2_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 2)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_2_0;\n",
      "    float w_1_0 = (((volatile float*)red_buf0)[((int)threadIdx.x)] + ((volatile float*)red_buf0)[(((int)threadIdx.x) + 1)]);\n",
      "    ((volatile float*)red_buf0)[((int)threadIdx.x)] = w_1_0;\n",
      "  }\n",
      "  __syncthreads();\n",
      "  if (((int)threadIdx.x) == 0) {\n",
      "    T_matmul_NT[0] = ((volatile float*)red_buf0)[0];\n",
      "  }\n",
      "  if (((int)threadIdx.x) == 0) {\n",
      "    T_relu[((int)blockIdx.x)] = max((T_matmul_NT[0] + placeholder2[((int)blockIdx.x)]), 0.000000e+00f);\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cuda Print\n",
    "source_module = lib.get_lib().imported_modules[0]\n",
    "\n",
    "#LLVM print\n",
    "#source_module = lib.get_lib()\n",
    "\n",
    "source_code = source_module.get_source()\n",
    "print(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1265af06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#runtime\n",
    "dev = tvm.cuda(0)\n",
    "tvm_exec = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "# Set inputs\n",
    "tvm_exec.set_input(input_name, tvm.nd.array(sample0.numpy().astype(dtype)))\n",
    "# Execute\n",
    "tvm_exec.run()\n",
    "# Get outputs\n",
    "tvm_output = tvm_exec.get_output(0)\n",
    "\n",
    "print(np.argmax(tvm_output.numpy()[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "greenhouse-wages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main([Var(actual_input_1, ty=TensorType([1, 1, 28, 28], float32))])\n",
      "`--Call \n",
      "   |--subtract \n",
      "   |--Call \n",
      "   |  |--subtract \n",
      "   |  |--Call \n",
      "   |  |  |--add \n",
      "   |  |  |--Call \n",
      "   |  |  |  |--nn.dense \n",
      "   |  |  |  |--Call \n",
      "   |  |  |  |  |--nn.relu \n",
      "   |  |  |  |  `--Call \n",
      "   |  |  |  |     |--add \n",
      "   |  |  |  |     |--Call \n",
      "   |  |  |  |     |  |--nn.dense \n",
      "   |  |  |  |     |  |--Call \n",
      "   |  |  |  |     |  |  |--nn.batch_flatten \n",
      "   |  |  |  |     |  |  `--Call \n",
      "   |  |  |  |     |  |     |--nn.max_pool2d \n",
      "   |  |  |  |     |  |     `--Call \n",
      "   |  |  |  |     |  |        |--nn.relu \n",
      "   |  |  |  |     |  |        `--Call \n",
      "   |  |  |  |     |  |           |--nn.bias_add \n",
      "   |  |  |  |     |  |           |--Call \n",
      "   |  |  |  |     |  |           |  |--nn.conv2d \n",
      "   |  |  |  |     |  |           |  |--Call \n",
      "   |  |  |  |     |  |           |  |  |--nn.relu \n",
      "   |  |  |  |     |  |           |  |  `--Call \n",
      "   |  |  |  |     |  |           |  |     |--nn.bias_add \n",
      "   |  |  |  |     |  |           |  |     |--Call \n",
      "   |  |  |  |     |  |           |  |     |  |--nn.conv2d \n",
      "   |  |  |  |     |  |           |  |     |  |--Var(Input) name_hint: actual_input_1, shape: (1, 1, 28, 28), dtype: float32\n",
      "   |  |  |  |     |  |           |  |     |  `--Const shape: (32, 1, 3, 3), dtype: float32\n",
      "   |  |  |  |     |  |           |  |     `--Const shape: (32,), dtype: float32\n",
      "   |  |  |  |     |  |           |  `--Const shape: (64, 32, 3, 3), dtype: float32\n",
      "   |  |  |  |     |  |           `--Const shape: (64,), dtype: float32\n",
      "   |  |  |  |     |  `--Const shape: (128, 9216), dtype: float32\n",
      "   |  |  |  |     `--Const shape: (128,), dtype: float32\n",
      "   |  |  |  `--Const shape: (10, 128), dtype: float32\n",
      "   |  |  `--Const shape: (10,), dtype: float32\n",
      "   |  `--Call \n",
      "   |     |--max \n",
      "   |     `--Call \n",
      "   |        |--add \n",
      "   |        |--Call \n",
      "   |        |  |--nn.dense \n",
      "   |        |  |--Call \n",
      "   |        |  |  |--nn.relu \n",
      "   |        |  |  `--Call \n",
      "   |        |  |     |--add \n",
      "   |        |  |     |--Call \n",
      "   |        |  |     |  |--nn.dense \n",
      "   |        |  |     |  |--Call \n",
      "   |        |  |     |  |  |--nn.batch_flatten \n",
      "   |        |  |     |  |  `--Call \n",
      "   |        |  |     |  |     |--nn.max_pool2d \n",
      "   |        |  |     |  |     `--Call \n",
      "   |        |  |     |  |        |--nn.relu \n",
      "   |        |  |     |  |        `--Call \n",
      "   |        |  |     |  |           |--nn.bias_add \n",
      "   |        |  |     |  |           |--Call \n",
      "   |        |  |     |  |           |  |--nn.conv2d \n",
      "   |        |  |     |  |           |  |--Call \n",
      "   |        |  |     |  |           |  |  |--nn.relu \n",
      "   |        |  |     |  |           |  |  `--Call \n",
      "   |        |  |     |  |           |  |     |--nn.bias_add \n",
      "   |        |  |     |  |           |  |     |--Call \n",
      "   |        |  |     |  |           |  |     |  |--nn.conv2d \n",
      "   |        |  |     |  |           |  |     |  |--Var(Input) name_hint: actual_input_1, shape: (1, 1, 28, 28), dtype: float32\n",
      "   |        |  |     |  |           |  |     |  `--Const shape: (32, 1, 3, 3), dtype: float32\n",
      "   |        |  |     |  |           |  |     `--Const shape: (32,), dtype: float32\n",
      "   |        |  |     |  |           |  `--Const shape: (64, 32, 3, 3), dtype: float32\n",
      "   |        |  |     |  |           `--Const shape: (64,), dtype: float32\n",
      "   |        |  |     |  `--Const shape: (128, 9216), dtype: float32\n",
      "   |        |  |     `--Const shape: (128,), dtype: float32\n",
      "   |        |  `--Const shape: (10, 128), dtype: float32\n",
      "   |        `--Const shape: (10,), dtype: float32\n",
      "   `--Call \n",
      "      |--log \n",
      "      `--Call \n",
      "         |--sum \n",
      "         `--Call \n",
      "            |--exp \n",
      "            `--Call \n",
      "               |--subtract \n",
      "               |--Call \n",
      "               |  |--add \n",
      "               |  |--Call \n",
      "               |  |  |--nn.dense \n",
      "               |  |  |--Call \n",
      "               |  |  |  |--nn.relu \n",
      "               |  |  |  `--Call \n",
      "               |  |  |     |--add \n",
      "               |  |  |     |--Call \n",
      "               |  |  |     |  |--nn.dense \n",
      "               |  |  |     |  |--Call \n",
      "               |  |  |     |  |  |--nn.batch_flatten \n",
      "               |  |  |     |  |  `--Call \n",
      "               |  |  |     |  |     |--nn.max_pool2d \n",
      "               |  |  |     |  |     `--Call \n",
      "               |  |  |     |  |        |--nn.relu \n",
      "               |  |  |     |  |        `--Call \n",
      "               |  |  |     |  |           |--nn.bias_add \n",
      "               |  |  |     |  |           |--Call \n",
      "               |  |  |     |  |           |  |--nn.conv2d \n",
      "               |  |  |     |  |           |  |--Call \n",
      "               |  |  |     |  |           |  |  |--nn.relu \n",
      "               |  |  |     |  |           |  |  `--Call \n",
      "               |  |  |     |  |           |  |     |--nn.bias_add \n",
      "               |  |  |     |  |           |  |     |--Call \n",
      "               |  |  |     |  |           |  |     |  |--nn.conv2d \n",
      "               |  |  |     |  |           |  |     |  |--Var(Input) name_hint: actual_input_1, shape: (1, 1, 28, 28), dtype: float32\n",
      "               |  |  |     |  |           |  |     |  `--Const shape: (32, 1, 3, 3), dtype: float32\n",
      "               |  |  |     |  |           |  |     `--Const shape: (32,), dtype: float32\n",
      "               |  |  |     |  |           |  `--Const shape: (64, 32, 3, 3), dtype: float32\n",
      "               |  |  |     |  |           `--Const shape: (64,), dtype: float32\n",
      "               |  |  |     |  `--Const shape: (128, 9216), dtype: float32\n",
      "               |  |  |     `--Const shape: (128,), dtype: float32\n",
      "               |  |  `--Const shape: (10, 128), dtype: float32\n",
      "               |  `--Const shape: (10,), dtype: float32\n",
      "               `--Call \n",
      "                  |--max \n",
      "                  `--Call \n",
      "                     |--add \n",
      "                     |--Call \n",
      "                     |  |--nn.dense \n",
      "                     |  |--Call \n",
      "                     |  |  |--nn.relu \n",
      "                     |  |  `--Call \n",
      "                     |  |     |--add \n",
      "                     |  |     |--Call \n",
      "                     |  |     |  |--nn.dense \n",
      "                     |  |     |  |--Call \n",
      "                     |  |     |  |  |--nn.batch_flatten \n",
      "                     |  |     |  |  `--Call \n",
      "                     |  |     |  |     |--nn.max_pool2d \n",
      "                     |  |     |  |     `--Call \n",
      "                     |  |     |  |        |--nn.relu \n",
      "                     |  |     |  |        `--Call \n",
      "                     |  |     |  |           |--nn.bias_add \n",
      "                     |  |     |  |           |--Call \n",
      "                     |  |     |  |           |  |--nn.conv2d \n",
      "                     |  |     |  |           |  |--Call \n",
      "                     |  |     |  |           |  |  |--nn.relu \n",
      "                     |  |     |  |           |  |  `--Call \n",
      "                     |  |     |  |           |  |     |--nn.bias_add \n",
      "                     |  |     |  |           |  |     |--Call \n",
      "                     |  |     |  |           |  |     |  |--nn.conv2d \n",
      "                     |  |     |  |           |  |     |  |--Var(Input) name_hint: actual_input_1, shape: (1, 1, 28, 28), dtype: float32\n",
      "                     |  |     |  |           |  |     |  `--Const shape: (32, 1, 3, 3), dtype: float32\n",
      "                     |  |     |  |           |  |     `--Const shape: (32,), dtype: float32\n",
      "                     |  |     |  |           |  `--Const shape: (64, 32, 3, 3), dtype: float32\n",
      "                     |  |     |  |           `--Const shape: (64,), dtype: float32\n",
      "                     |  |     |  `--Const shape: (128, 9216), dtype: float32\n",
      "                     |  |     `--Const shape: (128,), dtype: float32\n",
      "                     |  `--Const shape: (10, 128), dtype: float32\n",
      "                     `--Const shape: (10,), dtype: float32\n"
     ]
    }
   ],
   "source": [
    "#relay graphical print\n",
    "from typing import (\n",
    "    Dict,\n",
    "    Union,\n",
    "    Tuple,\n",
    "    List,\n",
    ")\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.contrib import relay_viz\n",
    "from tvm.contrib.relay_viz.interface import (\n",
    "    VizEdge,\n",
    "    VizNode,\n",
    "    VizParser,\n",
    ")\n",
    "from tvm.contrib.relay_viz.terminal import (\n",
    "    TermGraph,\n",
    "    TermPlotter,\n",
    "    TermVizParser,\n",
    ")\n",
    "\n",
    "viz = relay_viz.RelayVisualizer(mod)\n",
    "viz.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b8b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
